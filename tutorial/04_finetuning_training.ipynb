{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "import torch; torch.set_grad_enabled(False); # turn off gradients so memory doesn't explode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning for Semantic Segmentation\n",
    "\n",
    "Here we will understand the mechanics of fine-tuning the model for semantic segmentation. This is done by adding a new point feature upscaling and classification head to the end of the encoder.\n",
    "\n",
    "## Feature Upscaling\n",
    "\n",
    "The feature upscaling is done using a PointNet++ upscaling module. The basic procedure is as follows:\n",
    "\n",
    "1. Pass the point cloud through the encoder to get the token features at all intermediate layers.\n",
    "2. Compute the per-event global features by performing an average and max pooling of the token features across the intermediate layers (usually layers [3,7,11]), and concatenate the results.\n",
    "3. Treating each token from the last layer of the encoder as a point in 3D space, interpolate the latent features for all points in the cloud via inverse distance weighting.\n",
    "4. Concatenate the 3D positions of each point with its interpolated latent features, and pass the result through a MLP to encode positional context.\n",
    "5. Concatenate the 'global features' with the positional features, and pass the result through a MLP to encode positional context.\n",
    "\n",
    "## Segmentation Head\n",
    "\n",
    "The segmentation head is a simple MLP head that takes the upscaled features and outputs a per-point classification logits.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:polarmae.datasets.PILArNet:[rank: 0] self.emin=1e-06, self.emax=20.0, self.energy_threshold=0.13, self.remove_low_energy_scatters=True\n",
      "INFO:polarmae.datasets.PILArNet:[rank: 0] Building index\n",
      "INFO:polarmae.datasets.PILArNet:[rank: 0] 1045215 point clouds were loaded\n",
      "INFO:polarmae.datasets.PILArNet:[rank: 0] 10 files were loaded\n",
      "INFO:polarmae.datasets.PILArNet:[rank: 0] self.emin=1e-06, self.emax=20.0, self.energy_threshold=0.13, self.remove_low_energy_scatters=True\n",
      "INFO:polarmae.datasets.PILArNet:[rank: 0] Building index\n",
      "INFO:polarmae.datasets.PILArNet:[rank: 0] 10473 point clouds were loaded\n",
      "INFO:polarmae.datasets.PILArNet:[rank: 0] 1 files were loaded\n"
     ]
    }
   ],
   "source": [
    "from polarmae.datasets import PILArNetDataModule\n",
    "dataset = PILArNetDataModule(\n",
    "    data_path='/sdf/home/y/youngsam/data/dune/larnet/h5/DataAccessExamples/train/generic_v2*.h5',\n",
    "    batch_size=24,\n",
    "    num_workers=0,\n",
    "    dataset_kwargs={\n",
    "        'emin': 1.0e-6,                      # min energy for log transform\n",
    "        'emax': 20.0,                        # max energy for log transform\n",
    "        'energy_threshold': 0.13,            # remove points with energy < 0.13\n",
    "        'remove_low_energy_scatters': True,  # remove low energy scatters (PID=4)\n",
    "        'maxlen': -1,                        # max number of events to load\n",
    "        'min_points': 1024,\n",
    "    }\n",
    ")\n",
    "dataset.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will not apply HSTS. The HSTS database must be a regular and non-world-writable file.\n",
      "ERROR: could not open HSTS store at '/sdf/home/y/youngsam/.wget-hsts'. HSTS will be disabled.\n",
      "--2025-02-07 17:52:36--  https://github.com/DeepLearnPhysics/PoLAr-MAE/releases/download/weights/polarmae_pretrain.ckpt\n",
      "Resolving github.com (github.com)... 140.82.116.3\n",
      "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/927478490/ade5074b-3d24-4d8a-b0e0-65297a6fa9cd?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250208%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250208T015209Z&X-Amz-Expires=300&X-Amz-Signature=3743eb82ceb3fad54858cc9c520ce139f89a3e307b1938b5b3ecc3d1c5db40ba&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dpolarmae_pretrain.ckpt&response-content-type=application%2Foctet-stream [following]\n",
      "--2025-02-07 17:52:36--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/927478490/ade5074b-3d24-4d8a-b0e0-65297a6fa9cd?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250208%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250208T015209Z&X-Amz-Expires=300&X-Amz-Signature=3743eb82ceb3fad54858cc9c520ce139f89a3e307b1938b5b3ecc3d1c5db40ba&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dpolarmae_pretrain.ckpt&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 356044228 (340M) [application/octet-stream]\n",
      "Saving to: ‘polarmae_pretrain.ckpt.2’\n",
      "\n",
      "polarmae_pretrain.c  73%[=============>      ] 250.01M   107MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarmae_pretrain.c 100%[===================>] 339.55M  32.6MB/s    in 5.3s    \n",
      "\n",
      "2025-02-07 17:52:42 (64.5 MB/s) - ‘polarmae_pretrain.ckpt.2’ saved [356044228/356044228]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://github.com/DeepLearnPhysics/PoLAr-MAE/releases/download/weights/polarmae_pretrain.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sdf/home/y/youngsam/sw/dune/.conda/envs/py310_torch/lib/python3.12/site-packages/pytorch_lightning/utilities/parsing.py:209: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.\n",
      "/sdf/home/y/youngsam/sw/dune/.conda/envs/py310_torch/lib/python3.12/site-packages/pytorch_lightning/utilities/parsing.py:209: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.\n",
      "INFO:polarmae.models.ssl.polarmae:[rank: 0] ⚙️  MAE prediction: full patch reconstruction\n"
     ]
    }
   ],
   "source": [
    "from polarmae.models.ssl import PoLArMAE\n",
    "\n",
    "model = PoLArMAE.load_from_checkpoint(\"polarmae_pretrain.ckpt\").cuda()\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our point feature upsampler will do the following:\n",
    "\n",
    "\n",
    "Given some embeddings and centers and the points we will want to upscale to, we will\n",
    "\n",
    "1. Find the K nearest embeddings to each center\n",
    "2. Interpolate via inverse distance weighting to get embeddings for each point.\n",
    "3. Apply a 2 layer MLP with batch normalization to the embeddings\n",
    "\n",
    "\n",
    "In practice, the embeddings will actually be the average embeddings of a list of N layers in the encoder. For this we will use N=[3,7,11], and K=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polarmae.layers.feature_upsampling import PointNetFeatureUpsampling\n",
    "\n",
    "# Feature Upsampler ============================\n",
    "point_dim = 3\n",
    "upsampling_dim = model.encoder.embed_dim\n",
    "feature_upsampler = PointNetFeatureUpsampling(\n",
    "    in_channel=upsampling_dim,\n",
    "    mlp=[upsampling_dim, upsampling_dim],\n",
    ").cuda()\n",
    "\n",
    "batch = next(iter(dataset.train_dataloader()))\n",
    "points = batch['points'].cuda()\n",
    "lengths = batch['lengths'].cuda()\n",
    "labels = batch['semantic_id'].cuda().squeeze(-1)\n",
    "B, N, C = points.shape\n",
    "points = points.cuda()   # (B, N, 4)\n",
    "lengths = lengths.cuda() # (B,)\n",
    "labels = labels.cuda().squeeze(-1) # (B, N)\n",
    "\n",
    "out = model.encoder.prepare_tokens(points, lengths)\n",
    "output = model.encoder(out[\"x\"], out[\"pos_embed\"], out[\"emb_mask\"], return_hidden_states=True)\n",
    "batch_lengths = out[\"emb_mask\"].sum(dim=1)\n",
    "embeddings = output.last_hidden_state\n",
    "group_lengths = out[\"emb_mask\"].sum(dim=1)\n",
    "point_mask = torch.arange(lengths.max(), device=lengths.device).expand(\n",
    "            len(lengths), -1\n",
    "        ) < lengths.unsqueeze(-1)\n",
    "\n",
    "upsampled_features,_ = feature_upsampler(\n",
    "    points[..., :3],                 # xyz1\n",
    "    out['centers'][..., :3],       # xyz2\n",
    "    points[..., :3],                 # points1\n",
    "    embeddings,             # points2\n",
    "    lengths,                # point_lens\n",
    "    batch_lengths,          # embedding_lens\n",
    "    point_mask,             # point_mask for masked bn\n",
    ") # (B, N, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upsampled_features.shape torch.Size([24, 7839, 384])\n",
      "points.shape torch.Size([24, 7839, 4])\n"
     ]
    }
   ],
   "source": [
    "print('upsampled_features.shape', upsampled_features.shape)\n",
    "print('points.shape', points.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a latent feature for each individual point in the point cloud. Now we can perform point classification by running each point through a simple classification head!\n",
    "\n",
    "In Point-MAE/PoLAr-MAE, we actually concatenate along with the individual point features two global feature vectors that give a per-event summary of the point cloud. These correspond to the maximum and mean of the token features for each event. We first get the intermediate token features from the 3rd, 7th, and 11th layers of the encoder and take their mean. These features are then run through the masked mean/max functions and concatenated together.\n",
    "\n",
    "`TransformerEncoder.combine_intermediate_layers` does this fetching and averaging of the intermediate token features for us. Here's the code for it:\n",
    "\n",
    "```python\n",
    "    def combine_intermediate_layers(\n",
    "        self,\n",
    "        output: TransformerOutput,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "        layers: List[int] = [0],\n",
    "    ) -> torch.Tensor:\n",
    "        hidden_states = [\n",
    "            masked_layer_norm(output.hidden_states[i], output.hidden_states[i].shape[-1], mask)\n",
    "            for i in layers\n",
    "        ]\n",
    "        return torch.stack(hidden_states, dim=0).mean(0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upsampled_features.shape torch.Size([24, 7839, 1152])\n"
     ]
    }
   ],
   "source": [
    "def masked_mean(group, point_mask):\n",
    "    \"\"\"\n",
    "    perform a mean over the last dimension of the input,\n",
    "    taking care to only include valid points\n",
    "    \"\"\"\n",
    "    valid_elements = point_mask.sum(-1).float().clamp(min=1)\n",
    "    return (group * point_mask.unsqueeze(-1)).sum(-2) / valid_elements.unsqueeze(-1)\n",
    "\n",
    "def masked_max(group, point_mask):\n",
    "    \"\"\"\n",
    "    perform a max over the last dimension of the input,\n",
    "    taking care to only include valid points\n",
    "    \"\"\"\n",
    "    return (group - 1e10 * (~point_mask.unsqueeze(-1))).max(-2).values\n",
    "\n",
    "B, N, C = points.shape\n",
    "\n",
    "intermediate_features = model.encoder.combine_intermediate_layers(output, out[\"emb_mask\"], [3,7,11])\n",
    "\n",
    "global_feature = torch.cat(\n",
    "    [masked_max(intermediate_features, out[\"emb_mask\"]), masked_mean(intermediate_features, out[\"emb_mask\"])], dim=-1\n",
    ")\n",
    "upsampled_features = torch.cat(\n",
    "    [upsampled_features, global_feature.unsqueeze(-1).expand(-1, -1, N).transpose(1, 2)], dim=-1\n",
    ")\n",
    "print('upsampled_features.shape', upsampled_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a whopping 1152 (384 $\\times$ 3) features for each point in the point cloud!\n",
    "\n",
    "Our final segmentation head will be a 3 layer MLP with batch normalization and dropout. Each layer will downscale the feature dimension by 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of segmentation classes: 4\n",
      "First 10 predicted and true labels for the first event:\n",
      "tensor([1, 1, 3, 3, 1, 3, 1, 1, 1, 1], device='cuda:0') tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from polarmae.layers.seg_head import SegmentationHead\n",
    "\n",
    "seg_head_dim = 512\n",
    "num_seg_classes = dataset.num_seg_classes # 4 for us\n",
    "print('Number of segmentation classes:', num_seg_classes)\n",
    "seg_head_dropout = 0.5\n",
    "\n",
    "segmentation_head = SegmentationHead(\n",
    "    encoder_dim=model.encoder.embed_dim,\n",
    "    label_embedding_dim=0, # event-wide label embedding -- 0 for our dataset!\n",
    "    upsampling_dim=model.encoder.embed_dim,\n",
    "    seg_head_dim=seg_head_dim,\n",
    "    seg_head_dropout=seg_head_dropout,\n",
    "    num_seg_classes=num_seg_classes,\n",
    ").cuda()\n",
    "\n",
    "# note the need to transpose the features. this is because we use a 1D conv1d.\n",
    "cls_logits = segmentation_head(upsampled_features.transpose(1,2), point_mask).transpose(1,2)\n",
    "\n",
    "pred_label = torch.max(cls_logits, dim=-1).indices\n",
    "\n",
    "print('First 10 predicted and true labels for the first event:')\n",
    "print(pred_label[0, :10], labels.squeeze()[0, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance here is poor, but this is expected -- we haven't fine-tuned the model yet!\n",
    "\n",
    "___\n",
    "\n",
    "This entire model is encapsulated in `polarmae.models.finetune.semantic_segmentation.SemanticSegmentation`. This code is a little more complex than the other models because it has to handle the upscaling and the segmentation head and has more options (e.g., whether to do this global feature conditioning or use a transformer-based segmentation decoder after the encoder)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
